AWS Cloud Resume Challenge. 

https://cloudresumechallenge.dev/docs/the-challenge/aws/

Intro: This project is based on the the AWS Cloud Resume Challenge. The final product of the project is the website you are currently visiting. 

As per the challenge, the website should contain number of elements and technologies I described below. 
The aim of this project is to showcase the skills and knowledge of the AWS Cloud as well as some additional concepts related to networking, security, storage, web development and other.

To complete the challenge I ensured this website(my resume) included the AWS Cloud Practitioner certification. 
This was an introductory certification that provided me with fundamental knowledge of the industry-leading AWS cloud. 

As the challenge requires, I created my resume as a website using HTML instead of a simple Word document or PDF. 
To enhance its appearance, I styled it with CSS, keeping it simple but structured. 
I then deployed it as a static website on Amazon S3, opting for a hands-on approach rather than using abstracted services like Netlify or GitHub Pages. 
To optimize the website and provide elemntary level of security, I configured Amazon CloudFront to serve the site over HTTPS.
 
I created DNS domain name to my CloudFront distribution so that my resume is accessible at a URL like *my-c00l-resume-website.com*. 
I had used Amazon Route 53.

To reflect on some coding capabiliteis I included a visitor counter on my resume to track how many people had accessed the site. 
To implement this, I had written a small JavaScript with help of online tutorials. 
The visitor counter retrieves and updates its count from the AWS DynamoDB database.

9. **API**  
I had not communicated directly with DynamoDB from my JavaScript code. Instead, I had created an API that had accepted requests from my web app and had communicated with the database. I had used AWS’s API Gateway and Lambda services for this, as they had been free or nearly free for this project.  

10. **Python**  
I had written some code in my Lambda function. While I could have used JavaScript, I had chosen Python instead, as it had been commonly used for back-end programs and scripts. I had leveraged the `boto3` library for AWS. Here was a good, free Python tutorial I had referenced.  

11. **Tests**  
I had included tests for my Python code. Here were some resources I had used to write good Python tests.  

12. **Infrastructure as Code**  
I had not manually configured my API resources—such as the DynamoDB table, API Gateway, and Lambda function—by clicking around in the AWS console. Instead, I had defined them in an AWS Serverless Application Model (SAM) template and had deployed them using the AWS SAM CLI. This approach, known as “infrastructure as code” (IaC), had saved me time in the long run.  

13. **Source Control**  
I had ensured that my back-end API and front-end website had been updated automatically whenever I had made changes to the code. This process, called continuous integration and deployment (CI/CD), had allowed for seamless updates. I had created a GitHub repository for my back-end code.  

14. **CI/CD (Back end)**  
I had set up GitHub Actions so that when I had pushed updates to my Serverless Application Model template or Python code, my Python tests had run automatically. If the tests had passed, the SAM application had been packaged and deployed to AWS.  

15. **CI/CD (Front end)**  
I had created a second GitHub repository for my website code. I had also configured GitHub Actions to update my S3 bucket automatically whenever I had pushed new website code. Additionally, I had ensured that my CloudFront cache had been invalidated as needed. *Important note:* I had never committed AWS credentials to source control, as malicious actors could have found and misused them.  

16. **Blog Post**  
Finally, I had included a link in my resume to a short blog post describing some things I had learned while working on this project. I had published the post on Dev.to or Hashnode since I hadn’t needed my own blog for this.  
